# AgentForge Production Setup Guide
## Complete Step-by-Step Instructions for Million-Scale Deployment

This guide provides detailed instructions for setting up all external services and infrastructure required for your million-scale agent swarm system.

---

## üóÑÔ∏è 1. AWS RDS PostgreSQL Cluster Setup

### Step 1: Create AWS RDS Aurora PostgreSQL Cluster

1. **Go to AWS Console**: https://console.aws.amazon.com/rds/
2. **Click "Create database"**
3. **Choose database creation method**: Standard create
4. **Engine options**:
   - Engine type: Amazon Aurora
   - Edition: Amazon Aurora PostgreSQL-Compatible Edition
   - Version: PostgreSQL 15.4-R2 or latest
5. **Templates**: Production
6. **DB cluster identifier**: `agentforge-prod-cluster`
7. **Master username**: `agentforge`
8. **Master password**: Generate secure password and save it
9. **DB instance class**: db.r6g.2xlarge (or larger for higher performance)
10. **Availability & durability**: Multi-AZ deployment
11. **Connectivity**:
    - VPC: Create new or use existing
    - Subnet group: Create new
    - Public access: No (for security)
    - VPC security group: Create new with port 5432 open to your application subnets
12. **Additional configuration**:
    - Initial database name: `agentforge_prod`
    - Backup retention: 30 days
    - Monitoring: Enable Performance Insights
13. **Click "Create database"**

### Step 2: Create Read Replicas

1. **Select your cluster** in RDS console
2. **Actions ‚Üí Add reader**
3. **DB instance identifier**: `agentforge-prod-reader-1`
4. **DB instance class**: db.r6g.2xlarge
5. **Repeat for additional read replicas** (recommended: 2-3 readers)

### Step 3: Update Environment Variables

Replace in your `.env` file:
```bash
# Update these with your actual RDS endpoints
DATABASE_URL=postgresql://agentforge:YOUR_MASTER_PASSWORD@agentforge-prod-cluster.cluster-abc123.us-east-1.rds.amazonaws.com:5432/agentforge_prod
DATABASE_READER_URL=postgresql://agentforge:YOUR_MASTER_PASSWORD@agentforge-prod-cluster.cluster-ro-abc123.us-east-1.rds.amazonaws.com:5432/agentforge_prod
AWS_RDS_PASSWORD=YOUR_MASTER_PASSWORD
```

---

## üöÄ 2. AWS ElastiCache Redis Cluster Setup

### Step 1: Create ElastiCache Redis Cluster

1. **Go to AWS Console**: https://console.aws.amazon.com/elasticache/
2. **Click "Create"**
3. **Choose Redis**
4. **Cluster mode**: Enabled (for scaling)
5. **Location**: AWS Cloud
6. **Name**: `agentforge-redis`
7. **Description**: AgentForge Redis Cluster
8. **Engine version**: 7.0 or latest
9. **Port**: 6379
10. **Parameter group**: default.redis7.cluster.on
11. **Node type**: cache.r7g.2xlarge
12. **Number of shards**: 3
13. **Replicas per shard**: 2
14. **Subnet group**: Create new or use existing
15. **Security groups**: Create new allowing port 6379 from your application
16. **Encryption**: Enable both in-transit and at-rest
17. **Auth token**: Enable and generate secure token
18. **Click "Create"**

### Step 2: Update Environment Variables

```bash
# Update with your actual Redis endpoints
REDIS_URL=redis://agentforge-redis.abc123.cache.amazonaws.com:6379/0
REDIS_CLUSTER_ENABLED=true
REDIS_CLUSTER_NODES=agentforge-redis-0001-001.abc123.cache.amazonaws.com:6379,agentforge-redis-0001-002.abc123.cache.amazonaws.com:6379,agentforge-redis-0002-001.abc123.cache.amazonaws.com:6379
REDIS_PASSWORD=YOUR_AUTH_TOKEN
```

---

## üß† 3. Pinecone Vector Database Optimization

### Step 1: Optimize Pinecone Index

1. **Go to Pinecone Console**: https://app.pinecone.io/
2. **Select your existing index** or create new
3. **Index settings**:
   - Dimensions: 3072 (for OpenAI text-embedding-3-large)
   - Metric: cosine
   - Pod type: p2.x2 (performance optimized)
   - Pods: 4 (for high throughput)
   - Replicas: 2 (for high availability)
4. **Click "Create" or "Update"**

Your Pinecone is already configured in the environment file.

---

## üìä 4. Neo4j Knowledge Graph Database

### Step 1: Create Neo4j AuraDB Instance

1. **Go to Neo4j Aura**: https://console.neo4j.io/
2. **Click "Create new instance"**
3. **Choose AuraDB Professional**
4. **Instance name**: agentforge-knowledge-graph
5. **Dataset**: Start with blank database
6. **Cloud provider**: AWS (same region as your application)
7. **Instance size**: 
   - Memory: 32GB
   - Storage: 200GB
   - Compute: 8 vCPUs
8. **Click "Create instance"**
9. **Download credentials** (save the password securely)

### Step 2: Update Environment Variables

```bash
# Update with your actual Neo4j connection details
NEO4J_URI=neo4j+s://your-instance-id.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=YOUR_GENERATED_PASSWORD
```

---

## üì° 5. Apache Kafka Setup (Confluent Cloud)

### Step 1: Create Confluent Cloud Account

1. **Go to Confluent Cloud**: https://confluent.cloud/
2. **Sign up** for account
3. **Create new cluster**:
   - Cluster type: Basic (or Standard for production)
   - Cloud provider: AWS
   - Region: Same as your application
   - Cluster name: agentforge-kafka

### Step 2: Create API Keys

1. **In your cluster**, go to "API Keys"
2. **Create API Key**
3. **Scope**: Global access
4. **Save the API Key and Secret** securely

### Step 3: Create Topics

Create these topics in your cluster:
- `agent-executions`
- `swarm-coordination`
- `neural-mesh-updates`
- `knowledge-updates`
- `real-time-data`

### Step 4: Update Environment Variables

```bash
# Update with your actual Kafka details
KAFKA_BOOTSTRAP_SERVERS=pkc-your-cluster.us-east-1.aws.confluent.cloud:9092
KAFKA_API_KEY=YOUR_API_KEY
KAFKA_API_SECRET=YOUR_API_SECRET
```

---

## ü§ñ 6. AI/ML Services Setup

### Hugging Face

1. **Go to**: https://huggingface.co/settings/tokens
2. **Create new token** with "Read" permissions
3. **Update environment**:
```bash
HUGGINGFACE_API_TOKEN=hf_your_actual_token_here
```

### Replicate

1. **Go to**: https://replicate.com/account/api-tokens
2. **Create API token**
3. **Update environment**:
```bash
REPLICATE_API_TOKEN=r8_your_actual_token_here
```

### ElevenLabs

1. **Go to**: https://beta.elevenlabs.io/
2. **Sign up and go to Profile ‚Üí API Key**
3. **Copy API key**
4. **Update environment**:
```bash
ELEVENLABS_API_KEY=your_actual_api_key_here
```

### AssemblyAI

1. **Go to**: https://www.assemblyai.com/
2. **Sign up and get API key from dashboard**
3. **Update environment**:
```bash
ASSEMBLYAI_API_KEY=your_actual_api_key_here
```

### Mapbox

1. **Go to**: https://account.mapbox.com/access-tokens/
2. **Create new token** with all scopes
3. **Update environment**:
```bash
MAPBOX_ACCESS_TOKEN=pk.your_actual_token_here
```

### Wolfram Alpha

1. **Go to**: https://developer.wolframalpha.com/portal/myapps/
2. **Get an AppID**
3. **Update environment**:
```bash
WOLFRAM_APP_ID=your_actual_app_id_here
```

---

## üìà 7. Financial & Economic Data Services

### Alpha Vantage

1. **Go to**: https://www.alphavantage.co/support/#api-key
2. **Get free API key**
3. **Update environment**:
```bash
ALPHA_VANTAGE_API_KEY=your_actual_api_key_here
```

### FRED (Federal Reserve Economic Data)

1. **Go to**: https://fred.stlouisfed.org/docs/api/api_key.html
2. **Request API key**
3. **Update environment**:
```bash
FRED_API_KEY=your_actual_api_key_here
```

### Polygon.io

1. **Go to**: https://polygon.io/
2. **Sign up for account**
3. **Get API key from dashboard**
4. **Update environment**:
```bash
POLYGON_API_KEY=your_actual_api_key_here
```

### NewsAPI

1. **Go to**: https://newsapi.org/account
2. **Get API key**
3. **Update environment**:
```bash
NEWS_API_KEY=your_actual_api_key_here
```

---

## üõ∞Ô∏è 8. Satellite & Drone Data Integration

### Planet Labs

1. **Go to**: https://www.planet.com/account/#/
2. **Sign up for account**
3. **Get API key from account settings**
4. **Update environment**:
```bash
PLANET_API_KEY=your_actual_api_key_here
```

### NASA APIs

1. **Go to**: https://api.nasa.gov/
2. **Generate API key** (free)
3. **Update environment**:
```bash
NASA_API_KEY=your_actual_api_key_here
```

### NOAA

1. **Go to**: https://www.ncdc.noaa.gov/cdo-web/token
2. **Request API token**
3. **Update environment**:
```bash
NOAA_API_TOKEN=your_actual_api_token_here
```

---

## üìä 9. Monitoring & Observability

### Prometheus + Grafana (Kubernetes)

**Deploy using Helm**:

```bash
# Add Helm repositories
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# Install Prometheus
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi

# Get Grafana admin password
kubectl get secret --namespace monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 --decode
```

### Jaeger Tracing

```bash
# Install Jaeger
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
helm install jaeger jaegertracing/jaeger \
  --namespace tracing \
  --create-namespace
```

---

## üîç 10. Search Engines

### Elasticsearch (Elastic Cloud)

1. **Go to**: https://cloud.elastic.co/
2. **Create deployment**:
   - Cloud provider: AWS (same region)
   - Version: Latest 8.x
   - Hardware profile: I/O Optimized
   - Size: 8GB RAM, 2 zones
3. **Save credentials** from deployment overview
4. **Update environment**:
```bash
ELASTICSEARCH_URL=https://your-deployment-id.es.us-east-1.aws.found.io:9243
ELASTICSEARCH_USERNAME=elastic
ELASTICSEARCH_PASSWORD=your_generated_password
```

### Algolia

1. **Go to**: https://www.algolia.com/
2. **Sign up and create application**
3. **Go to API Keys** in dashboard
4. **Update environment**:
```bash
ALGOLIA_APPLICATION_ID=your_app_id
ALGOLIA_API_KEY=your_admin_api_key
ALGOLIA_SEARCH_API_KEY=your_search_api_key
```

---

## üîê 11. AWS Secrets Manager

### Step 1: Create Secret

```bash
# Using AWS CLI
aws secretsmanager create-secret \
  --name "agentforge/production/secrets" \
  --description "AgentForge production secrets" \
  --secret-string '{
    "database_password": "YOUR_RDS_PASSWORD",
    "redis_password": "YOUR_REDIS_PASSWORD",
    "jwt_secret": "YOUR_JWT_SECRET",
    "encryption_key": "YOUR_ENCRYPTION_KEY"
  }'
```

### Step 2: Update IAM Role

Ensure your application's IAM role has `secretsmanager:GetSecretValue` permission.

---

## üîë 12. Auth0 with Multi-Factor Authentication

### Step 1: Create Auth0 Account

1. **Go to**: https://auth0.com/
2. **Sign up** for account
3. **Create new tenant**: `your-company-name`

### Step 2: Create Application

1. **Applications ‚Üí Create Application**
2. **Name**: AgentForge API
3. **Type**: Machine to Machine
4. **Authorize**: Auth0 Management API
5. **Scopes**: Select all user management scopes

### Step 3: Configure MFA

1. **Security ‚Üí Multi-factor Auth**
2. **Enable**: One-time Password, SMS, Push notifications
3. **Configure policies** for when MFA is required

### Step 4: Update Environment Variables

```bash
AUTH0_DOMAIN=your-tenant.us.auth0.com
AUTH0_CLIENT_ID=your_actual_client_id
AUTH0_CLIENT_SECRET=your_actual_client_secret
AUTH0_MANAGEMENT_API_TOKEN=your_management_api_token
```

---

## ‚òÅÔ∏è 13. CloudFlare CDN Setup

### Step 1: Add Domain to CloudFlare

1. **Go to**: https://dash.cloudflare.com/
2. **Add site**: your-domain.com
3. **Choose plan**: Pro or Business
4. **Update nameservers** at your domain registrar

### Step 2: Create API Token

1. **My Profile ‚Üí API Tokens**
2. **Create Token**: Custom token
3. **Permissions**:
   - Zone:Zone:Edit
   - Zone:DNS:Edit
   - Account:Cloudflare Workers:Edit
4. **Update environment**:
```bash
CLOUDFLARE_API_TOKEN=your_actual_api_token
CLOUDFLARE_ZONE_ID=your_zone_id
CLOUDFLARE_ACCOUNT_ID=your_account_id
```

---

## üöÄ 14. Final Deployment Steps

### Step 1: Update All Environment Variables

Copy all the updated values from this guide into your actual `.env` file.

### Step 2: Deploy Infrastructure

```bash
# Deploy to Kubernetes with updated configuration
kubectl apply -f deployment/k8s/production/

# Verify all services are running
kubectl get pods -n agentforge-production
```

### Step 3: Initialize Databases

```bash
# Run database migrations
python -m core.database_manager

# Initialize Pinecone index
python scripts/initialize_pinecone.py

# Seed Neo4j with initial knowledge
python scripts/initialize_knowledge_graph.py
```

### Step 4: Verify Services

```bash
# Test all external service connections
python scripts/test_external_services.py

# Run health checks
curl https://your-domain.com/health
```

---

## üéØ Performance Optimization Checklist

- [ ] **Database**: Connection pooling configured (500+ connections)
- [ ] **Redis**: Cluster mode enabled with 3 shards
- [ ] **Pinecone**: Performance pods (p2.x2) with 4 pods
- [ ] **Neo4j**: Professional tier with 32GB memory
- [ ] **Kafka**: Standard cluster with multiple partitions
- [ ] **Monitoring**: Prometheus + Grafana deployed
- [ ] **Tracing**: Jaeger collecting 10% of traces
- [ ] **CDN**: CloudFlare caching static assets
- [ ] **Search**: Elasticsearch with I/O optimized instances
- [ ] **Security**: All services using encrypted connections

---

## üìû Support & Troubleshooting

### Common Issues

1. **Database Connection Timeouts**
   - Increase `DB_POOL_TIMEOUT` in environment
   - Verify security group allows port 5432

2. **Redis Cluster Connection Issues**
   - Ensure all cluster nodes are listed in `REDIS_CLUSTER_NODES`
   - Verify auth token is correct

3. **External API Rate Limits**
   - Implement exponential backoff
   - Consider upgrading to higher tier plans

### Monitoring Dashboards

Access your monitoring at:
- **Grafana**: `https://grafana.your-domain.com`
- **Jaeger**: `https://jaeger.your-domain.com`
- **Prometheus**: `https://prometheus.your-domain.com`

---

## üéâ Completion

Once all services are configured and deployed, your AgentForge system will have:

- **Million-scale agent coordination** capability
- **Real-time intelligence** from 15+ external data sources
- **Production-grade infrastructure** with high availability
- **Comprehensive monitoring** and observability
- **Enterprise security** with MFA and encryption
- **Global CDN** for optimal performance

Your agents will now operate at SME level with access to:
- Real-time financial markets data
- Satellite and geospatial intelligence  
- Advanced AI/ML models
- Computational mathematics (Wolfram)
- Global news and information
- Weather and environmental data
- Voice synthesis and speech recognition

**Total setup time**: 4-6 hours
**Monthly cost estimate**: $2,000-5,000 (depending on usage)
**Agent capacity**: 1,000,000+ concurrent agents
**Response time**: <100ms for cached queries, <2s for complex operations
